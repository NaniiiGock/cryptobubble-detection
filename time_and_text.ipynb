{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilianahotsko/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(folder_path):\n",
    "    tweet_embeddings = {}\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    counter = 0\n",
    "    for crypto in os.listdir(folder_path):\n",
    "        if counter <=3:\n",
    "            crypto_path = os.path.join(folder_path, crypto)\n",
    "            print(crypto_path)\n",
    "            if os.path.isdir(crypto_path):  \n",
    "                embeddings = []\n",
    "                for file in os.listdir(crypto_path):\n",
    "                    if file.endswith('.csv'):\n",
    "                        file_path = os.path.join(crypto_path, file)\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        if 'tweet' not in df.columns:\n",
    "                            continue\n",
    "                        tweets = df['tweet'].astype(str).tolist()\n",
    "\n",
    "    \n",
    "                        inputs = tokenizer(tweets, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(**inputs)\n",
    "                        tweet_embeds = outputs.last_hidden_state.mean(dim=1)  # Mean over tokens\n",
    "                        embeddings.append(tweet_embeds.mean(dim=0).numpy())  # Aggregate daily embeddings\n",
    "                \n",
    "                if embeddings:\n",
    "                    tweet_embeddings[crypto] = np.mean(embeddings, axis=0)  # Average across days\n",
    "                counter += 1\n",
    "    return tweet_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_price_data(folder_path):\n",
    "    time_series_data = {}\n",
    "    scaler = MinMaxScaler()\n",
    "    counter = 0\n",
    "    for file in os.listdir(folder_path):\n",
    "        if counter <=10:\n",
    "            if file.endswith('.csv'):\n",
    "                crypto_name = os.path.splitext(file)[0]\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                features = ['low_x', 'high_x', 'open_x', 'close_x', 'volumefrom_x', 'volumeto_x']\n",
    "                if set(features).issubset(df.columns):\n",
    "                    df[features] = scaler.fit_transform(df[features])\n",
    "                    time_series_data[crypto_name] = df[features].values\n",
    "                counter +=1\n",
    "    return time_series_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_data(tweet_embeddings, price_data, labels):\n",
    "    cryptos = set(tweet_embeddings.keys()).intersection(price_data.keys())\n",
    "    tweet_features = []\n",
    "    time_features = []\n",
    "    aligned_labels = []\n",
    "\n",
    "    for crypto in cryptos:\n",
    "        tweet_features.append(tweet_embeddings[crypto])\n",
    "        time_features.append(price_data[crypto][:len(tweet_embeddings[crypto])])\n",
    "        aligned_labels.append(labels[crypto][:len(tweet_embeddings[crypto])])\n",
    "\n",
    "    return np.array(tweet_features), np.array(time_features), np.array(aligned_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_time_series(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    features = ['low_x', 'high_x', 'open_x', 'close_x', 'volumefrom_x', 'volumeto_x']\n",
    "    scaler = MinMaxScaler()\n",
    "    df[features] = scaler.fit_transform(df[features])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoDataset(Dataset):\n",
    "    def __init__(self, tweet_features, time_series, labels):\n",
    "        self.tweet_features = tweet_features\n",
    "        self.time_series = time_series\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.tweet_features[idx], dtype=torch.float32),\n",
    "                torch.tensor(self.time_series[idx], dtype=torch.float32),\n",
    "                torch.tensor(self.labels[idx], dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, text_dim, time_dim, hidden_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.fc_text = nn.Linear(text_dim, hidden_dim)\n",
    "        self.fc_time = nn.Linear(time_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, text_features, time_series):\n",
    "        text_out = torch.relu(self.fc_text(text_features))\n",
    "        time_out = torch.relu(self.fc_time(time_series))\n",
    "        combined = torch.cat((text_out, time_out), dim=1)\n",
    "        output = torch.sigmoid(self.fc_out(combined))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = load_price_data('data/price_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HBAR': array([[0.        , 0.0749275 , 0.        , 0.06519687, 0.02090418,\n",
       "         0.00360096],\n",
       "        [0.09139827, 0.08268327, 0.08385683, 0.07091276, 0.03367995,\n",
       "         0.00638974],\n",
       "        [0.09728108, 0.0659353 , 0.08945863, 0.07336242, 0.01336238,\n",
       "         0.00243983],\n",
       "        ...,\n",
       "        [0.84163804, 0.71427287, 0.8071883 , 0.7841231 , 0.13104733,\n",
       "         0.19662959],\n",
       "        [0.84470491, 0.73630376, 0.7884323 , 0.8063232 , 0.18123067,\n",
       "         0.27376609],\n",
       "        [0.77221528, 0.71697053, 0.81018926, 0.72619868, 0.10239891,\n",
       "         0.14778306]], shape=(556, 6)),\n",
       " 'IOTA': array([[0.0971885 , 0.0880139 , 0.09611972, 0.0844079 , 0.01685332,\n",
       "         0.00275412],\n",
       "        [0.10038339, 0.07695426, 0.0844079 , 0.08677393, 0.00667513,\n",
       "         0.00103991],\n",
       "        [0.10115016, 0.08697163, 0.08677393, 0.09606057, 0.00746058,\n",
       "         0.0012243 ],\n",
       "        ...,\n",
       "        [0.90415335, 0.95830921, 0.85626405, 0.95741157, 0.19829879,\n",
       "         0.202999  ],\n",
       "        [1.        , 0.99826288, 0.95741157, 1.        , 0.14680962,\n",
       "         0.15640928],\n",
       "        [0.9456869 , 0.98089172, 1.        , 0.89234591, 0.11706719,\n",
       "         0.12013749]], shape=(532, 6)),\n",
       " 'WAN': array([[0.17996861, 0.17635252, 0.19753086, 0.16341095, 0.02958395,\n",
       "         0.00671372],\n",
       "        [0.15272951, 0.15605328, 0.16341095, 0.1306975 , 0.05889592,\n",
       "         0.01147065],\n",
       "        [0.12324851, 0.10888605, 0.1306975 , 0.10428713, 0.03134533,\n",
       "         0.00508987],\n",
       "        ...,\n",
       "        [0.96076673, 0.95438372, 0.91040267, 1.        , 0.18120287,\n",
       "         0.17497996],\n",
       "        [1.        , 0.94526047, 1.        , 0.93905298, 0.11961837,\n",
       "         0.11933705],\n",
       "        [0.86660688, 0.82757048, 0.93905298, 0.82809814, 0.12521728,\n",
       "         0.10997715]], shape=(589, 6)),\n",
       " 'KSM': array([[0.02238431, 0.03525724, 0.03590551, 0.03356426, 0.2827341 ,\n",
       "         0.0495095 ],\n",
       "        [0.01170484, 0.02915538, 0.03356426, 0.01230114, 0.40798991,\n",
       "         0.06122278],\n",
       "        [0.01307847, 0.01964094, 0.01230114, 0.02061064, 0.21378563,\n",
       "         0.03084986],\n",
       "        ...,\n",
       "        [1.        , 0.94983123, 0.98629795, 0.98629795, 0.        ,\n",
       "         0.        ],\n",
       "        [1.        , 0.94983123, 0.98629795, 0.98629795, 0.        ,\n",
       "         0.        ],\n",
       "        [0.68874787, 0.94983123, 0.98629795, 0.76295841, 0.44048582,\n",
       "         0.79664937]], shape=(216, 6)),\n",
       " 'LTO': array([[0.        , 0.13942067, 0.        , 0.11723009, 1.        ,\n",
       "         0.38570096],\n",
       "        [0.11432861, 0.11304625, 0.14197578, 0.08300965, 0.4437994 ,\n",
       "         0.16202858],\n",
       "        [0.09384529, 0.07787277, 0.1087146 , 0.06346169, 0.2508383 ,\n",
       "         0.07292551],\n",
       "        ...,\n",
       "        [0.89812533, 0.91678414, 0.85626778, 0.91689815, 0.06524195,\n",
       "         0.18541295],\n",
       "        [0.96447633, 0.98726752, 0.91922765, 1.        , 0.07002871,\n",
       "         0.21171791],\n",
       "        [0.85489483, 1.        , 1.        , 0.86618411, 0.07480474,\n",
       "         0.21615791]], shape=(425, 6)),\n",
       " 'CREAM': array([[0.27430469, 0.28787225, 0.3312771 , 0.30547429, 0.05171838,\n",
       "         0.05497282],\n",
       "        [0.33142918, 0.32815422, 0.32080792, 0.33437908, 0.05670303,\n",
       "         0.07041508],\n",
       "        [0.17937092, 0.31868796, 0.33437908, 0.19404984, 0.22490336,\n",
       "         0.22531741],\n",
       "        ...,\n",
       "        [0.39701525, 0.30620055, 0.3855617 , 0.3855617 , 0.        ,\n",
       "         0.        ],\n",
       "        [0.39701525, 0.30620055, 0.3855617 , 0.3855617 , 0.        ,\n",
       "         0.        ],\n",
       "        [0.39701525, 0.43795137, 0.3855617 , 0.42567591, 0.09795716,\n",
       "         0.1454884 ]], shape=(219, 6)),\n",
       " 'OCEAN': array([[0.        , 0.00438322, 0.01370078, 0.        , 0.38089718,\n",
       "         0.07908894],\n",
       "        [0.00455263, 0.        , 0.        , 0.00715165, 0.10960743,\n",
       "         0.0227361 ],\n",
       "        [0.00601225, 0.00995617, 0.00715165, 0.01952951, 0.10796632,\n",
       "         0.02434072],\n",
       "        ...,\n",
       "        [1.        , 0.88791484, 0.93778325, 0.93778325, 0.        ,\n",
       "         0.        ],\n",
       "        [1.        , 0.88791484, 0.93778325, 0.93778325, 0.        ,\n",
       "         0.        ],\n",
       "        [0.94926081, 0.98872887, 0.93778325, 0.92403007, 0.05272351,\n",
       "         0.17911389]], shape=(274, 6)),\n",
       " 'OF': array([[0.        , 0.60745829, 0.        , 0.69715324, 0.00639599,\n",
       "         0.00739963],\n",
       "        [0.68727914, 0.60745829, 0.70568672, 0.75772259, 0.01017935,\n",
       "         0.01187585],\n",
       "        [0.74982331, 0.70559372, 0.76454938, 0.8788613 , 0.06179752,\n",
       "         0.07703902],\n",
       "        ...,\n",
       "        [0.05620845, 0.01422964, 0.05289987, 0.02543913, 0.        ,\n",
       "         0.        ],\n",
       "        [0.05620845, 0.01422964, 0.05289987, 0.02543913, 0.        ,\n",
       "         0.        ],\n",
       "        [0.05620845, 0.01422964, 0.05289987, 0.02543913, 0.        ,\n",
       "         0.        ]], shape=(1085, 6)),\n",
       " 'SASHIMI': array([[0.97243427, 0.93219616, 1.        , 0.80585454, 0.06762076,\n",
       "         0.17084148],\n",
       "        [1.        , 1.        , 0.60930964, 0.93293767, 0.18864597,\n",
       "         0.53644986],\n",
       "        [0.71713316, 0.58230277, 0.70539766, 0.57851323, 0.24589536,\n",
       "         0.4482177 ],\n",
       "        ...,\n",
       "        [0.28977947, 0.13226013, 0.17052455, 0.22553063, 0.        ,\n",
       "         0.        ],\n",
       "        [0.28977947, 0.13226013, 0.17052455, 0.22553063, 0.        ,\n",
       "         0.        ],\n",
       "        [0.23676845, 0.13226013, 0.17052455, 0.18666801, 0.3661296 ,\n",
       "         0.22883976]], shape=(208, 6)),\n",
       " 'SAND': array([[0.01504365, 0.0718804 , 0.01143503, 0.04410656, 0.94867527,\n",
       "         0.49255903],\n",
       "        [0.0271323 , 0.04461069, 0.04410656, 0.02538326, 0.35035256,\n",
       "         0.14974471],\n",
       "        [0.02578912, 0.04364984, 0.02538326, 0.03763508, 0.27547656,\n",
       "         0.12102479],\n",
       "        ...,\n",
       "        [0.85963734, 0.82853482, 0.86453883, 0.8053531 , 0.04379305,\n",
       "         0.27246422],\n",
       "        [0.76977837, 0.74366014, 0.8053531 , 0.76024127, 0.04197957,\n",
       "         0.2462827 ],\n",
       "        [0.67320349, 0.70454   , 0.76024127, 0.67868811, 0.05764437,\n",
       "         0.28448932]], shape=(237, 6)),\n",
       " 'XRPBULL': array([[9.54839388e-01, 8.62319787e-01, 8.76799691e-01, 8.66390253e-01,\n",
       "         6.55849101e-05, 1.01664423e-02],\n",
       "        [1.00000000e+00, 9.61607997e-01, 8.66390253e-01, 1.00000000e+00,\n",
       "         1.06669483e-05, 1.90944270e-03],\n",
       "        [8.65885074e-01, 1.00000000e+00, 1.00000000e+00, 8.00332048e-01,\n",
       "         6.69702281e-05, 1.15802018e-02],\n",
       "        ...,\n",
       "        [1.64554966e-03, 9.77710724e-04, 1.05851246e-03, 1.05851246e-03,\n",
       "         0.00000000e+00, 0.00000000e+00],\n",
       "        [1.64554966e-03, 9.77710724e-04, 1.05851246e-03, 1.05851246e-03,\n",
       "         0.00000000e+00, 0.00000000e+00],\n",
       "        [1.64554966e-03, 8.12511326e-03, 1.05851246e-03, 3.13161570e-03,\n",
       "         3.25616848e-01, 3.54171264e-01]], shape=(415, 6))}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/tweets/EOSBULL\n",
      "data/tweets/CELO\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tweet_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mload_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/tweets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 23\u001b[0m, in \u001b[0;36mload_tweets\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(tweets, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m tweet_embeds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Mean over tokens\u001b[39;00m\n\u001b[1;32m     25\u001b[0m embeddings\u001b[38;5;241m.\u001b[39mappend(tweet_embeds\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# Aggregate daily embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/transformers/pytorch_utils.py:258\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    554\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tweet_embeddings = load_tweets('data/tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tweet_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mload_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/tweets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m price_data \u001b[38;5;241m=\u001b[39m load_price_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/price_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assume labels is a dictionary with crypto names and corresponding label arrays\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m, in \u001b[0;36mload_tweets\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(tweets, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 21\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m tweet_embeds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Mean over tokens\u001b[39;00m\n\u001b[1;32m     23\u001b[0m embeddings\u001b[38;5;241m.\u001b[39mappend(tweet_embeds\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# Aggregate daily embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:408\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    406\u001b[0m     key_layer, value_layer \u001b[38;5;241m=\u001b[39m past_key_value\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    409\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(current_states))\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention:\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "labels = {crypto: np.random.randint(0, 2, len(price_data[crypto])) for crypto in price_data.keys()}  # Example labels\n",
    "\n",
    "X_tweets, X_time, y = align_data(tweet_embeddings, price_data, labels)\n",
    "\n",
    "X_tweets_train, X_tweets_test, X_time_train, X_time_test, y_train, y_test = train_test_split(\n",
    "    X_tweets, X_time, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = CryptoDataset(X_tweets_train, X_time_train, y_train)\n",
    "test_dataset = CryptoDataset(X_tweets_test, X_time_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CombinedModel(text_dim=768, time_dim=X_time_train.shape[2], hidden_dim=128)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for tweet_batch, time_batch, label_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(tweet_batch, time_batch).squeeze()\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss/len(train_loader)}\")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for tweet_batch, time_batch, label_batch in test_loader:\n",
    "        preds = model(tweet_batch, time_batch).squeeze()\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(label_batch.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_preds_binary = (all_preds > 0.5).astype(int)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds_binary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_dataset(price_data_folder, tweets_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for crypto_file in os.listdir(price_data_folder):\n",
    "        if crypto_file.endswith('.csv'):\n",
    "            crypto_name = os.path.splitext(crypto_file)[0]\n",
    "            price_file_path = os.path.join(price_data_folder, crypto_file)\n",
    "            tweet_folder_path = os.path.join(tweets_folder, crypto_name)\n",
    "\n",
    "            if not os.path.exists(tweet_folder_path):\n",
    "                print(f\"No tweets folder for cryptocurrency: {crypto_name}\")\n",
    "                continue\n",
    "\n",
    "            price_data = pd.read_csv(price_file_path)\n",
    "            if 'datetime' not in price_data.columns:\n",
    "                print(f\"'datetime' column missing in price data for {crypto_name}\")\n",
    "                continue\n",
    "\n",
    "            price_data['datetime'] = pd.to_datetime(price_data['datetime']).dt.date\n",
    "            combined_data = []\n",
    "\n",
    "            for _, row in price_data.iterrows():\n",
    "                date = row['datetime']\n",
    "                tweet_file = os.path.join(tweet_folder_path, f\"{date}.csv\")\n",
    "                if os.path.exists(tweet_file):\n",
    "                    try:\n",
    "                        tweets_df = pd.read_csv(tweet_file, engine='python', on_bad_lines='skip')\n",
    "                        if 'tweet' in tweets_df.columns:\n",
    "                            tweets_list = tweets_df['tweet'].tolist()\n",
    "                            row['tweets'] = tweets_list\n",
    "                        else:\n",
    "                            row['tweets'] = []\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {tweet_file}: {e}\")\n",
    "                        row['tweets'] = []\n",
    "                else:\n",
    "                    row['tweets'] = []\n",
    "                combined_data.append(row)\n",
    "\n",
    "            combined_df = pd.DataFrame(combined_data)\n",
    "            output_path = os.path.join(output_folder, f\"{crypto_name}_combined.csv\")\n",
    "            combined_df.to_csv(output_path, index=False)\n",
    "            print(f\"Combined dataset saved for {crypto_name} at {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved for HBAR at data/combined_data/HBAR_combined.csv\n",
      "Combined dataset saved for IOTA at data/combined_data/IOTA_combined.csv\n",
      "Combined dataset saved for WAN at data/combined_data/WAN_combined.csv\n",
      "Combined dataset saved for KSM at data/combined_data/KSM_combined.csv\n",
      "Combined dataset saved for LTO at data/combined_data/LTO_combined.csv\n",
      "Combined dataset saved for CREAM at data/combined_data/CREAM_combined.csv\n",
      "Combined dataset saved for OCEAN at data/combined_data/OCEAN_combined.csv\n",
      "Combined dataset saved for OF at data/combined_data/OF_combined.csv\n",
      "Combined dataset saved for SASHIMI at data/combined_data/SASHIMI_combined.csv\n",
      "Combined dataset saved for SAND at data/combined_data/SAND_combined.csv\n",
      "Combined dataset saved for XRPBULL at data/combined_data/XRPBULL_combined.csv\n",
      "Combined dataset saved for KTON at data/combined_data/KTON_combined.csv\n",
      "Combined dataset saved for DMD at data/combined_data/DMD_combined.csv\n",
      "Combined dataset saved for DRGN at data/combined_data/DRGN_combined.csv\n",
      "Combined dataset saved for CKB at data/combined_data/CKB_combined.csv\n",
      "Combined dataset saved for XSR at data/combined_data/XSR_combined.csv\n",
      "Combined dataset saved for XRA at data/combined_data/XRA_combined.csv\n",
      "Combined dataset saved for DREP at data/combined_data/DREP_combined.csv\n",
      "Combined dataset saved for MDT at data/combined_data/MDT_combined.csv\n",
      "Combined dataset saved for XTZUP at data/combined_data/XTZUP_combined.csv\n",
      "Combined dataset saved for HNS at data/combined_data/HNS_combined.csv\n",
      "Combined dataset saved for STORJ at data/combined_data/STORJ_combined.csv\n",
      "Combined dataset saved for MX at data/combined_data/MX_combined.csv\n",
      "Combined dataset saved for TOMO at data/combined_data/TOMO_combined.csv\n",
      "Combined dataset saved for CRV at data/combined_data/CRV_combined.csv\n",
      "Combined dataset saved for SUN at data/combined_data/SUN_combined.csv\n",
      "Combined dataset saved for WAX at data/combined_data/WAX_combined.csv\n",
      "Combined dataset saved for BCH at data/combined_data/BCH_combined.csv\n",
      "Combined dataset saved for LINKDOWN at data/combined_data/LINKDOWN_combined.csv\n",
      "Combined dataset saved for LBA at data/combined_data/LBA_combined.csv\n",
      "Combined dataset saved for SENC at data/combined_data/SENC_combined.csv\n",
      "Combined dataset saved for BTT at data/combined_data/BTT_combined.csv\n",
      "Combined dataset saved for NAX at data/combined_data/NAX_combined.csv\n",
      "Combined dataset saved for BTC at data/combined_data/BTC_combined.csv\n",
      "Combined dataset saved for OPEN at data/combined_data/OPEN_combined.csv\n",
      "Combined dataset saved for OGO at data/combined_data/OGO_combined.csv\n",
      "Combined dataset saved for AMPL at data/combined_data/AMPL_combined.csv\n",
      "Combined dataset saved for BU at data/combined_data/BU_combined.csv\n",
      "Combined dataset saved for MDA at data/combined_data/MDA_combined.csv\n",
      "Combined dataset saved for QUN at data/combined_data/QUN_combined.csv\n",
      "Combined dataset saved for CHZ at data/combined_data/CHZ_combined.csv\n",
      "Combined dataset saved for DMG at data/combined_data/DMG_combined.csv\n",
      "Combined dataset saved for ORN at data/combined_data/ORN_combined.csv\n",
      "Combined dataset saved for LAMB at data/combined_data/LAMB_combined.csv\n",
      "Combined dataset saved for SOC at data/combined_data/SOC_combined.csv\n",
      "Combined dataset saved for HOT at data/combined_data/HOT_combined.csv\n",
      "Combined dataset saved for KAVA at data/combined_data/KAVA_combined.csv\n",
      "Combined dataset saved for EOSUP at data/combined_data/EOSUP_combined.csv\n",
      "Combined dataset saved for OGN at data/combined_data/OGN_combined.csv\n",
      "Error reading data/tweets/TRX/2019-12-22.csv: No columns to parse from file\n",
      "Combined dataset saved for TRX at data/combined_data/TRX_combined.csv\n",
      "Combined dataset saved for ERD at data/combined_data/ERD_combined.csv\n",
      "Combined dataset saved for BAT at data/combined_data/BAT_combined.csv\n",
      "Combined dataset saved for PPT at data/combined_data/PPT_combined.csv\n",
      "Combined dataset saved for SUSHI at data/combined_data/SUSHI_combined.csv\n",
      "Combined dataset saved for LTCUP at data/combined_data/LTCUP_combined.csv\n",
      "Combined dataset saved for ETHBEAR at data/combined_data/ETHBEAR_combined.csv\n",
      "Combined dataset saved for SWRV at data/combined_data/SWRV_combined.csv\n",
      "Combined dataset saved for RED at data/combined_data/RED_combined.csv\n",
      "Combined dataset saved for BCX at data/combined_data/BCX_combined.csv\n",
      "Combined dataset saved for NANO at data/combined_data/NANO_combined.csv\n",
      "Combined dataset saved for PHA at data/combined_data/PHA_combined.csv\n",
      "Combined dataset saved for ESS at data/combined_data/ESS_combined.csv\n",
      "Combined dataset saved for UTK at data/combined_data/UTK_combined.csv\n",
      "Combined dataset saved for IHT at data/combined_data/IHT_combined.csv\n",
      "Combined dataset saved for HNT at data/combined_data/HNT_combined.csv\n",
      "Combined dataset saved for RUNE at data/combined_data/RUNE_combined.csv\n",
      "Combined dataset saved for MDS at data/combined_data/MDS_combined.csv\n",
      "Combined dataset saved for EOSBULL at data/combined_data/EOSBULL_combined.csv\n",
      "Combined dataset saved for SOP at data/combined_data/SOP_combined.csv\n",
      "Combined dataset saved for SNT at data/combined_data/SNT_combined.csv\n",
      "Combined dataset saved for XRP at data/combined_data/XRP_combined.csv\n",
      "Combined dataset saved for XPO at data/combined_data/XPO_combined.csv\n",
      "Combined dataset saved for UNIUP at data/combined_data/UNIUP_combined.csv\n",
      "Combined dataset saved for APM at data/combined_data/APM_combined.csv\n",
      "Combined dataset saved for FTT at data/combined_data/FTT_combined.csv\n",
      "Combined dataset saved for FRONT at data/combined_data/FRONT_combined.csv\n",
      "Combined dataset saved for AGS at data/combined_data/AGS_combined.csv\n",
      "Combined dataset saved for DTA at data/combined_data/DTA_combined.csv\n",
      "Combined dataset saved for VIB at data/combined_data/VIB_combined.csv\n",
      "Combined dataset saved for BTG at data/combined_data/BTG_combined.csv\n",
      "Combined dataset saved for BEAR at data/combined_data/BEAR_combined.csv\n",
      "Combined dataset saved for BCN at data/combined_data/BCN_combined.csv\n",
      "Combined dataset saved for RIF at data/combined_data/RIF_combined.csv\n",
      "Combined dataset saved for BOX at data/combined_data/BOX_combined.csv\n",
      "Combined dataset saved for GSE at data/combined_data/GSE_combined.csv\n",
      "Combined dataset saved for TRADE at data/combined_data/TRADE_combined.csv\n",
      "Combined dataset saved for FIRO at data/combined_data/FIRO_combined.csv\n",
      "Combined dataset saved for RATING at data/combined_data/RATING_combined.csv\n",
      "Combined dataset saved for REP at data/combined_data/REP_combined.csv\n",
      "Combined dataset saved for LYM at data/combined_data/LYM_combined.csv\n",
      "Combined dataset saved for HBC at data/combined_data/HBC_combined.csv\n",
      "Combined dataset saved for VET at data/combined_data/VET_combined.csv\n",
      "Combined dataset saved for BTMX at data/combined_data/BTMX_combined.csv\n",
      "Combined dataset saved for ZSC at data/combined_data/ZSC_combined.csv\n",
      "Combined dataset saved for ZEN at data/combined_data/ZEN_combined.csv\n",
      "Combined dataset saved for MET at data/combined_data/MET_combined.csv\n",
      "Combined dataset saved for AST at data/combined_data/AST_combined.csv\n",
      "Combined dataset saved for SAN at data/combined_data/SAN_combined.csv\n",
      "Combined dataset saved for JNT at data/combined_data/JNT_combined.csv\n",
      "Combined dataset saved for ZIL at data/combined_data/ZIL_combined.csv\n",
      "Combined dataset saved for GUSD at data/combined_data/GUSD_combined.csv\n",
      "Combined dataset saved for STX at data/combined_data/STX_combined.csv\n",
      "Combined dataset saved for XEM at data/combined_data/XEM_combined.csv\n",
      "Combined dataset saved for APIX at data/combined_data/APIX_combined.csv\n",
      "Combined dataset saved for STEEM at data/combined_data/STEEM_combined.csv\n",
      "Combined dataset saved for CHAT at data/combined_data/CHAT_combined.csv\n",
      "Combined dataset saved for GMAT at data/combined_data/GMAT_combined.csv\n",
      "Combined dataset saved for BTS at data/combined_data/BTS_combined.csv\n",
      "Combined dataset saved for DOTDOWN at data/combined_data/DOTDOWN_combined.csv\n",
      "Combined dataset saved for NBS at data/combined_data/NBS_combined.csv\n",
      "Combined dataset saved for EGT at data/combined_data/EGT_combined.csv\n",
      "Combined dataset saved for EDO at data/combined_data/EDO_combined.csv\n",
      "Combined dataset saved for REQ at data/combined_data/REQ_combined.csv\n",
      "Combined dataset saved for XAUT at data/combined_data/XAUT_combined.csv\n",
      "Combined dataset saved for ETHBULL at data/combined_data/ETHBULL_combined.csv\n",
      "Combined dataset saved for LTCDOWN at data/combined_data/LTCDOWN_combined.csv\n",
      "Combined dataset saved for GXS at data/combined_data/GXS_combined.csv\n",
      "Combined dataset saved for QASH at data/combined_data/QASH_combined.csv\n",
      "Combined dataset saved for BLOC at data/combined_data/BLOC_combined.csv\n",
      "Combined dataset saved for STRAX at data/combined_data/STRAX_combined.csv\n",
      "Combined dataset saved for OAX at data/combined_data/OAX_combined.csv\n",
      "Combined dataset saved for PAXG at data/combined_data/PAXG_combined.csv\n",
      "Combined dataset saved for ACH at data/combined_data/ACH_combined.csv\n",
      "Combined dataset saved for BNBDOWN at data/combined_data/BNBDOWN_combined.csv\n",
      "Combined dataset saved for MTL at data/combined_data/MTL_combined.csv\n",
      "Combined dataset saved for MOF at data/combined_data/MOF_combined.csv\n",
      "Combined dataset saved for DGD at data/combined_data/DGD_combined.csv\n",
      "Combined dataset saved for EOSBEAR at data/combined_data/EOSBEAR_combined.csv\n",
      "Combined dataset saved for CELO at data/combined_data/CELO_combined.csv\n",
      "Combined dataset saved for ATOM at data/combined_data/ATOM_combined.csv\n",
      "Combined dataset saved for AUD at data/combined_data/AUD_combined.csv\n",
      "Combined dataset saved for RING at data/combined_data/RING_combined.csv\n",
      "Combined dataset saved for HIT at data/combined_data/HIT_combined.csv\n",
      "Combined dataset saved for FIL at data/combined_data/FIL_combined.csv\n",
      "Combined dataset saved for LUNA at data/combined_data/LUNA_combined.csv\n",
      "Combined dataset saved for BULL at data/combined_data/BULL_combined.csv\n",
      "Combined dataset saved for ANKR at data/combined_data/ANKR_combined.csv\n",
      "Combined dataset saved for POA at data/combined_data/POA_combined.csv\n",
      "Combined dataset saved for ENJ at data/combined_data/ENJ_combined.csv\n",
      "Combined dataset saved for PVT at data/combined_data/PVT_combined.csv\n",
      "Combined dataset saved for HIVE at data/combined_data/HIVE_combined.csv\n",
      "Combined dataset saved for WRX at data/combined_data/WRX_combined.csv\n",
      "Combined dataset saved for NODE at data/combined_data/NODE_combined.csv\n",
      "Combined dataset saved for RVC at data/combined_data/RVC_combined.csv\n",
      "Combined dataset saved for CTXC at data/combined_data/CTXC_combined.csv\n",
      "Combined dataset saved for LSK at data/combined_data/LSK_combined.csv\n",
      "Combined dataset saved for DILI at data/combined_data/DILI_combined.csv\n",
      "Combined dataset saved for IQ at data/combined_data/IQ_combined.csv\n",
      "Combined dataset saved for DPY at data/combined_data/DPY_combined.csv\n",
      "Combined dataset saved for ONT at data/combined_data/ONT_combined.csv\n",
      "Combined dataset saved for TNT at data/combined_data/TNT_combined.csv\n",
      "Combined dataset saved for CNN at data/combined_data/CNN_combined.csv\n",
      "Combined dataset saved for MAN at data/combined_data/MAN_combined.csv\n",
      "Combined dataset saved for HPT at data/combined_data/HPT_combined.csv\n",
      "Combined dataset saved for HPB at data/combined_data/HPB_combined.csv\n",
      "Combined dataset saved for DOTUP at data/combined_data/DOTUP_combined.csv\n",
      "Combined dataset saved for CMT at data/combined_data/CMT_combined.csv\n",
      "Combined dataset saved for XMC at data/combined_data/XMC_combined.csv\n",
      "Combined dataset saved for POWR at data/combined_data/POWR_combined.csv\n",
      "Combined dataset saved for HC at data/combined_data/HC_combined.csv\n",
      "Combined dataset saved for FIO at data/combined_data/FIO_combined.csv\n",
      "Combined dataset saved for HT at data/combined_data/HT_combined.csv\n",
      "Combined dataset saved for NKN at data/combined_data/NKN_combined.csv\n",
      "Combined dataset saved for SERO at data/combined_data/SERO_combined.csv\n",
      "Combined dataset saved for LRN at data/combined_data/LRN_combined.csv\n",
      "Combined dataset saved for GAS at data/combined_data/GAS_combined.csv\n",
      "Combined dataset saved for SWAP at data/combined_data/SWAP_combined.csv\n",
      "Combined dataset saved for ETP at data/combined_data/ETP_combined.csv\n",
      "Combined dataset saved for ELA at data/combined_data/ELA_combined.csv\n",
      "Combined dataset saved for KAI at data/combined_data/KAI_combined.csv\n",
      "Combined dataset saved for LET at data/combined_data/LET_combined.csv\n",
      "Combined dataset saved for BIX at data/combined_data/BIX_combined.csv\n",
      "Combined dataset saved for PEARL at data/combined_data/PEARL_combined.csv\n",
      "Combined dataset saved for BEL at data/combined_data/BEL_combined.csv\n",
      "Combined dataset saved for GNX at data/combined_data/GNX_combined.csv\n",
      "Combined dataset saved for ETC at data/combined_data/ETC_combined.csv\n",
      "Combined dataset saved for COMP at data/combined_data/COMP_combined.csv\n",
      "Combined dataset saved for BSV at data/combined_data/BSV_combined.csv\n",
      "Combined dataset saved for GNO at data/combined_data/GNO_combined.csv\n",
      "Combined dataset saved for WAVES at data/combined_data/WAVES_combined.csv\n",
      "Combined dataset saved for MANA at data/combined_data/MANA_combined.csv\n",
      "Combined dataset saved for XAS at data/combined_data/XAS_combined.csv\n",
      "Combined dataset saved for ALV at data/combined_data/ALV_combined.csv\n",
      "Combined dataset saved for XRPBEAR at data/combined_data/XRPBEAR_combined.csv\n",
      "Combined dataset saved for AUC at data/combined_data/AUC_combined.csv\n",
      "Combined dataset saved for DKA at data/combined_data/DKA_combined.csv\n",
      "Combined dataset saved for JFI at data/combined_data/JFI_combined.csv\n",
      "Combined dataset saved for ATP at data/combined_data/ATP_combined.csv\n",
      "Combined dataset saved for DGB at data/combined_data/DGB_combined.csv\n",
      "Combined dataset saved for TAI at data/combined_data/TAI_combined.csv\n",
      "Combined dataset saved for NEST at data/combined_data/NEST_combined.csv\n",
      "Combined dataset saved for SKM at data/combined_data/SKM_combined.csv\n",
      "Combined dataset saved for MINI at data/combined_data/MINI_combined.csv\n",
      "Combined dataset saved for PNT at data/combined_data/PNT_combined.csv\n",
      "Combined dataset saved for YFV at data/combined_data/YFV_combined.csv\n",
      "Combined dataset saved for NEW at data/combined_data/NEW_combined.csv\n",
      "Error reading data/tweets/KNC/2020-04-03.csv: No columns to parse from file\n",
      "Combined dataset saved for KNC at data/combined_data/KNC_combined.csv\n",
      "Combined dataset saved for GXC at data/combined_data/GXC_combined.csv\n",
      "Combined dataset saved for KAN at data/combined_data/KAN_combined.csv\n",
      "Combined dataset saved for ELF at data/combined_data/ELF_combined.csv\n",
      "Combined dataset saved for TFUEL at data/combined_data/TFUEL_combined.csv\n",
      "Combined dataset saved for RNT at data/combined_data/RNT_combined.csv\n",
      "Combined dataset saved for BNTY at data/combined_data/BNTY_combined.csv\n",
      "Combined dataset saved for IOST at data/combined_data/IOST_combined.csv\n",
      "Combined dataset saved for INT at data/combined_data/INT_combined.csv\n",
      "Combined dataset saved for PAY at data/combined_data/PAY_combined.csv\n",
      "Combined dataset saved for BCDN at data/combined_data/BCDN_combined.csv\n",
      "Combined dataset saved for SRM at data/combined_data/SRM_combined.csv\n",
      "Combined dataset saved for DUSK at data/combined_data/DUSK_combined.csv\n",
      "Combined dataset saved for GRIN at data/combined_data/GRIN_combined.csv\n",
      "Combined dataset saved for ONE at data/combined_data/ONE_combined.csv\n",
      "Combined dataset saved for AERGO at data/combined_data/AERGO_combined.csv\n",
      "Combined dataset saved for TCT at data/combined_data/TCT_combined.csv\n",
      "Combined dataset saved for XUC at data/combined_data/XUC_combined.csv\n",
      "Combined dataset saved for MLN at data/combined_data/MLN_combined.csv\n",
      "Combined dataset saved for TOPC at data/combined_data/TOPC_combined.csv\n",
      "Combined dataset saved for DF at data/combined_data/DF_combined.csv\n",
      "Combined dataset saved for XMR at data/combined_data/XMR_combined.csv\n",
      "Combined dataset saved for AMP at data/combined_data/AMP_combined.csv\n",
      "Combined dataset saved for EUR at data/combined_data/EUR_combined.csv\n",
      "Combined dataset saved for NEC at data/combined_data/NEC_combined.csv\n",
      "Combined dataset saved for AION at data/combined_data/AION_combined.csv\n",
      "Combined dataset saved for DATA at data/combined_data/DATA_combined.csv\n",
      "Combined dataset saved for GTC at data/combined_data/GTC_combined.csv\n",
      "Combined dataset saved for BHD at data/combined_data/BHD_combined.csv\n",
      "Combined dataset saved for ETHDOWN at data/combined_data/ETHDOWN_combined.csv\n",
      "Combined dataset saved for ALY at data/combined_data/ALY_combined.csv\n",
      "Combined dataset saved for ADAUP at data/combined_data/ADAUP_combined.csv\n",
      "Combined dataset saved for OCN at data/combined_data/OCN_combined.csv\n",
      "Combined dataset saved for WAXP at data/combined_data/WAXP_combined.csv\n",
      "Combined dataset saved for NULS at data/combined_data/NULS_combined.csv\n",
      "Combined dataset saved for QSP at data/combined_data/QSP_combined.csv\n",
      "Combined dataset saved for BNBBULL at data/combined_data/BNBBULL_combined.csv\n",
      "Combined dataset saved for CND at data/combined_data/CND_combined.csv\n",
      "Combined dataset saved for XRPUP at data/combined_data/XRPUP_combined.csv\n",
      "Combined dataset saved for SC at data/combined_data/SC_combined.csv\n",
      "Combined dataset saved for DHT at data/combined_data/DHT_combined.csv\n",
      "Combined dataset saved for ADEL at data/combined_data/ADEL_combined.csv\n",
      "Combined dataset saved for MITH at data/combined_data/MITH_combined.csv\n",
      "Combined dataset saved for XLM at data/combined_data/XLM_combined.csv\n",
      "Combined dataset saved for CGLD at data/combined_data/CGLD_combined.csv\n",
      "Combined dataset saved for CVC at data/combined_data/CVC_combined.csv\n",
      "Combined dataset saved for CVT at data/combined_data/CVT_combined.csv\n",
      "Combined dataset saved for DOCK at data/combined_data/DOCK_combined.csv\n",
      "Combined dataset saved for SBTC at data/combined_data/SBTC_combined.csv\n",
      "Combined dataset saved for NEO at data/combined_data/NEO_combined.csv\n",
      "Combined dataset saved for WIN at data/combined_data/WIN_combined.csv\n",
      "Combined dataset saved for GTO at data/combined_data/GTO_combined.csv\n",
      "Combined dataset saved for BHP at data/combined_data/BHP_combined.csv\n",
      "Combined dataset saved for LEO at data/combined_data/LEO_combined.csv\n",
      "Combined dataset saved for COTI at data/combined_data/COTI_combined.csv\n",
      "Combined dataset saved for FUEL at data/combined_data/FUEL_combined.csv\n",
      "Combined dataset saved for ALGO at data/combined_data/ALGO_combined.csv\n",
      "Combined dataset saved for GNT at data/combined_data/GNT_combined.csv\n",
      "Combined dataset saved for KLAY at data/combined_data/KLAY_combined.csv\n",
      "Combined dataset saved for EOSDAC at data/combined_data/EOSDAC_combined.csv\n",
      "Combined dataset saved for AVAX at data/combined_data/AVAX_combined.csv\n",
      "Combined dataset saved for IRIS at data/combined_data/IRIS_combined.csv\n",
      "Combined dataset saved for CLO at data/combined_data/CLO_combined.csv\n",
      "Combined dataset saved for MCO at data/combined_data/MCO_combined.csv\n",
      "Combined dataset saved for BTSE at data/combined_data/BTSE_combined.csv\n",
      "Combined dataset saved for SFG at data/combined_data/SFG_combined.csv\n",
      "Combined dataset saved for CELR at data/combined_data/CELR_combined.csv\n",
      "Combined dataset saved for DEP at data/combined_data/DEP_combined.csv\n",
      "Combined dataset saved for CAI at data/combined_data/CAI_combined.csv\n",
      "Combined dataset saved for BZRX at data/combined_data/BZRX_combined.csv\n",
      "Combined dataset saved for ACT at data/combined_data/ACT_combined.csv\n",
      "Combined dataset saved for OMG at data/combined_data/OMG_combined.csv\n",
      "Combined dataset saved for EM at data/combined_data/EM_combined.csv\n",
      "Combined dataset saved for QKC at data/combined_data/QKC_combined.csv\n",
      "Combined dataset saved for BTCDOWN at data/combined_data/BTCDOWN_combined.csv\n",
      "Combined dataset saved for PLG at data/combined_data/PLG_combined.csv\n",
      "Combined dataset saved for GOF at data/combined_data/GOF_combined.csv\n",
      "Combined dataset saved for USDG at data/combined_data/USDG_combined.csv\n",
      "Combined dataset saved for ITC at data/combined_data/ITC_combined.csv\n",
      "Combined dataset saved for VTHO at data/combined_data/VTHO_combined.csv\n",
      "Combined dataset saved for ROAD at data/combined_data/ROAD_combined.csv\n",
      "Combined dataset saved for EOS at data/combined_data/EOS_combined.csv\n",
      "Combined dataset saved for BKRW at data/combined_data/BKRW_combined.csv\n",
      "Combined dataset saved for LIEN at data/combined_data/LIEN_combined.csv\n",
      "Combined dataset saved for RCN at data/combined_data/RCN_combined.csv\n",
      "Combined dataset saved for YFI at data/combined_data/YFI_combined.csv\n",
      "Combined dataset saved for PNK at data/combined_data/PNK_combined.csv\n",
      "Combined dataset saved for TRXUP at data/combined_data/TRXUP_combined.csv\n",
      "Combined dataset saved for YEE at data/combined_data/YEE_combined.csv\n",
      "Combined dataset saved for VSYS at data/combined_data/VSYS_combined.csv\n",
      "Combined dataset saved for LRC at data/combined_data/LRC_combined.csv\n",
      "Combined dataset saved for THETA at data/combined_data/THETA_combined.csv\n",
      "Combined dataset saved for ANW at data/combined_data/ANW_combined.csv\n",
      "Combined dataset saved for ABT at data/combined_data/ABT_combined.csv\n",
      "Combined dataset saved for NBOT at data/combined_data/NBOT_combined.csv\n",
      "Combined dataset saved for VIDY at data/combined_data/VIDY_combined.csv\n",
      "Combined dataset saved for SWFTC at data/combined_data/SWFTC_combined.csv\n",
      "Combined dataset saved for HSC at data/combined_data/HSC_combined.csv\n",
      "Combined dataset saved for BTCUP at data/combined_data/BTCUP_combined.csv\n",
      "Combined dataset saved for XTZ at data/combined_data/XTZ_combined.csv\n",
      "Combined dataset saved for RINGX at data/combined_data/RINGX_combined.csv\n",
      "Combined dataset saved for PI at data/combined_data/PI_combined.csv\n",
      "Combined dataset saved for DIA at data/combined_data/DIA_combined.csv\n",
      "Combined dataset saved for OXT at data/combined_data/OXT_combined.csv\n",
      "Combined dataset saved for RVN at data/combined_data/RVN_combined.csv\n",
      "Combined dataset saved for ICX at data/combined_data/ICX_combined.csv\n",
      "Combined dataset saved for GOT at data/combined_data/GOT_combined.csv\n",
      "Combined dataset saved for UFR at data/combined_data/UFR_combined.csv\n",
      "Combined dataset saved for WICC at data/combined_data/WICC_combined.csv\n",
      "Combined dataset saved for ORBS at data/combined_data/ORBS_combined.csv\n",
      "Combined dataset saved for UGC at data/combined_data/UGC_combined.csv\n",
      "Combined dataset saved for KMD at data/combined_data/KMD_combined.csv\n",
      "Combined dataset saved for GLM at data/combined_data/GLM_combined.csv\n",
      "Combined dataset saved for PCX at data/combined_data/PCX_combined.csv\n",
      "Combined dataset saved for GARD at data/combined_data/GARD_combined.csv\n",
      "Combined dataset saved for MXT at data/combined_data/MXT_combined.csv\n",
      "Combined dataset saved for YAMV2 at data/combined_data/YAMV2_combined.csv\n",
      "Combined dataset saved for MXC at data/combined_data/MXC_combined.csv\n",
      "Combined dataset saved for CVP at data/combined_data/CVP_combined.csv\n",
      "Combined dataset saved for ANT at data/combined_data/ANT_combined.csv\n",
      "Combined dataset saved for ONL at data/combined_data/ONL_combined.csv\n",
      "Combined dataset saved for TRXDOWN at data/combined_data/TRXDOWN_combined.csv\n",
      "Combined dataset saved for DENT at data/combined_data/DENT_combined.csv\n",
      "Combined dataset saved for DDD at data/combined_data/DDD_combined.csv\n",
      "Combined dataset saved for XVG at data/combined_data/XVG_combined.csv\n",
      "Combined dataset saved for UNIDOWN at data/combined_data/UNIDOWN_combined.csv\n",
      "Combined dataset saved for KCASH at data/combined_data/KCASH_combined.csv\n",
      "Combined dataset saved for YFII at data/combined_data/YFII_combined.csv\n",
      "Combined dataset saved for ETHUP at data/combined_data/ETHUP_combined.csv\n",
      "Combined dataset saved for FSN at data/combined_data/FSN_combined.csv\n",
      "Combined dataset saved for MBL at data/combined_data/MBL_combined.csv\n",
      "Combined dataset saved for MTA at data/combined_data/MTA_combined.csv\n",
      "Combined dataset saved for GT at data/combined_data/GT_combined.csv\n",
      "Combined dataset saved for FET at data/combined_data/FET_combined.csv\n",
      "Combined dataset saved for MTV at data/combined_data/MTV_combined.csv\n",
      "Combined dataset saved for DX at data/combined_data/DX_combined.csv\n",
      "Combined dataset saved for STPT at data/combined_data/STPT_combined.csv\n",
      "Combined dataset saved for JST at data/combined_data/JST_combined.csv\n",
      "Combined dataset saved for RUFF at data/combined_data/RUFF_combined.csv\n",
      "Combined dataset saved for GBP at data/combined_data/GBP_combined.csv\n",
      "Combined dataset saved for ETH at data/combined_data/ETH_combined.csv\n",
      "Combined dataset saved for NDN at data/combined_data/NDN_combined.csv\n",
      "Combined dataset saved for RLC at data/combined_data/RLC_combined.csv\n",
      "Combined dataset saved for WING at data/combined_data/WING_combined.csv\n",
      "Combined dataset saved for BOT at data/combined_data/BOT_combined.csv\n",
      "Combined dataset saved for LBK at data/combined_data/LBK_combined.csv\n",
      "Combined dataset saved for LOL at data/combined_data/LOL_combined.csv\n",
      "Combined dataset saved for RRB at data/combined_data/RRB_combined.csv\n",
      "Combined dataset saved for ELEC at data/combined_data/ELEC_combined.csv\n",
      "Combined dataset saved for QTUM at data/combined_data/QTUM_combined.csv\n",
      "Combined dataset saved for TMTG at data/combined_data/TMTG_combined.csv\n",
      "Combined dataset saved for DTX at data/combined_data/DTX_combined.csv\n",
      "Combined dataset saved for BNBUP at data/combined_data/BNBUP_combined.csv\n",
      "Combined dataset saved for ZPT at data/combined_data/ZPT_combined.csv\n",
      "Combined dataset saved for OIN at data/combined_data/OIN_combined.csv\n",
      "Combined dataset saved for FTM at data/combined_data/FTM_combined.csv\n",
      "Combined dataset saved for ARK at data/combined_data/ARK_combined.csv\n",
      "Combined dataset saved for XRPDOWN at data/combined_data/XRPDOWN_combined.csv\n",
      "Combined dataset saved for CIC at data/combined_data/CIC_combined.csv\n",
      "Combined dataset saved for MFT at data/combined_data/MFT_combined.csv\n",
      "Combined dataset saved for DNA at data/combined_data/DNA_combined.csv\n",
      "Combined dataset saved for COCOS at data/combined_data/COCOS_combined.csv\n",
      "Combined dataset saved for ADADOWN at data/combined_data/ADADOWN_combined.csv\n",
      "Combined dataset saved for DBC at data/combined_data/DBC_combined.csv\n",
      "Combined dataset saved for MKR at data/combined_data/MKR_combined.csv\n",
      "Combined dataset saved for STMX at data/combined_data/STMX_combined.csv\n",
      "Combined dataset saved for ZEC at data/combined_data/ZEC_combined.csv\n",
      "Combined dataset saved for DOS at data/combined_data/DOS_combined.csv\n",
      "Combined dataset saved for AE at data/combined_data/AE_combined.csv\n",
      "Combined dataset saved for AR at data/combined_data/AR_combined.csv\n",
      "Combined dataset saved for YOU at data/combined_data/YOU_combined.csv\n",
      "Combined dataset saved for NAS at data/combined_data/NAS_combined.csv\n",
      "Combined dataset saved for RRT at data/combined_data/RRT_combined.csv\n",
      "Combined dataset saved for PRA at data/combined_data/PRA_combined.csv\n",
      "Combined dataset saved for UNI at data/combined_data/UNI_combined.csv\n",
      "Combined dataset saved for EGLD at data/combined_data/EGLD_combined.csv\n",
      "Combined dataset saved for DASH at data/combined_data/DASH_combined.csv\n",
      "Combined dataset saved for RSR at data/combined_data/RSR_combined.csv\n",
      "Combined dataset saved for ARPA at data/combined_data/ARPA_combined.csv\n",
      "Combined dataset saved for NMR at data/combined_data/NMR_combined.csv\n",
      "Combined dataset saved for SWM at data/combined_data/SWM_combined.csv\n",
      "Combined dataset saved for AGI at data/combined_data/AGI_combined.csv\n",
      "Combined dataset saved for SMT at data/combined_data/SMT_combined.csv\n",
      "Combined dataset saved for VITE at data/combined_data/VITE_combined.csv\n",
      "Combined dataset saved for SNX at data/combined_data/SNX_combined.csv\n",
      "Combined dataset saved for LOOM at data/combined_data/LOOM_combined.csv\n",
      "Combined dataset saved for MIX at data/combined_data/MIX_combined.csv\n",
      "Combined dataset saved for UBTC at data/combined_data/UBTC_combined.csv\n",
      "Combined dataset saved for DCR at data/combined_data/DCR_combined.csv\n",
      "Combined dataset saved for CHR at data/combined_data/CHR_combined.csv\n",
      "Combined dataset saved for LINKUP at data/combined_data/LINKUP_combined.csv\n",
      "Combined dataset saved for FOR at data/combined_data/FOR_combined.csv\n",
      "Combined dataset saved for NPXS at data/combined_data/NPXS_combined.csv\n",
      "Combined dataset saved for OM at data/combined_data/OM_combined.csv\n",
      "Combined dataset saved for WGRT at data/combined_data/WGRT_combined.csv\n",
      "Combined dataset saved for TRUE at data/combined_data/TRUE_combined.csv\n",
      "Combined dataset saved for RFR at data/combined_data/RFR_combined.csv\n",
      "Combined dataset saved for RFUEL at data/combined_data/RFUEL_combined.csv\n",
      "Combined dataset saved for BLZ at data/combined_data/BLZ_combined.csv\n",
      "Combined dataset saved for ZYRO at data/combined_data/ZYRO_combined.csv\n",
      "Combined dataset saved for ARDR at data/combined_data/ARDR_combined.csv\n",
      "Combined dataset saved for WXT at data/combined_data/WXT_combined.csv\n",
      "Combined dataset saved for REM at data/combined_data/REM_combined.csv\n",
      "Combined dataset saved for WBTC at data/combined_data/WBTC_combined.csv\n",
      "Combined dataset saved for PERL at data/combined_data/PERL_combined.csv\n",
      "Combined dataset saved for BEAM at data/combined_data/BEAM_combined.csv\n",
      "Combined dataset saved for BTO at data/combined_data/BTO_combined.csv\n",
      "Combined dataset saved for WNXM at data/combined_data/WNXM_combined.csv\n",
      "Combined dataset saved for SNET at data/combined_data/SNET_combined.csv\n",
      "Combined dataset saved for TRB at data/combined_data/TRB_combined.csv\n",
      "Combined dataset saved for CRO at data/combined_data/CRO_combined.csv\n",
      "Combined dataset saved for FLM at data/combined_data/FLM_combined.csv\n",
      "Combined dataset saved for SALT at data/combined_data/SALT_combined.csv\n",
      "Combined dataset saved for EOSDOWN at data/combined_data/EOSDOWN_combined.csv\n",
      "Combined dataset saved for SXP at data/combined_data/SXP_combined.csv\n",
      "Combined dataset saved for TT at data/combined_data/TT_combined.csv\n",
      "Combined dataset saved for NEXO at data/combined_data/NEXO_combined.csv\n",
      "Combined dataset saved for DOT at data/combined_data/DOT_combined.csv\n",
      "Combined dataset saved for XTZDOWN at data/combined_data/XTZDOWN_combined.csv\n",
      "Combined dataset saved for BNBBEAR at data/combined_data/BNBBEAR_combined.csv\n",
      "Combined dataset saved for CDT at data/combined_data/CDT_combined.csv\n",
      "Combined dataset saved for DAI at data/combined_data/DAI_combined.csv\n",
      "Combined dataset saved for FUN at data/combined_data/FUN_combined.csv\n",
      "Combined dataset saved for ADA at data/combined_data/ADA_combined.csv\n",
      "Combined dataset saved for DTH at data/combined_data/DTH_combined.csv\n",
      "Combined dataset saved for RSV at data/combined_data/RSV_combined.csv\n",
      "Combined dataset saved for IOTX at data/combined_data/IOTX_combined.csv\n",
      "Combined dataset saved for AKRO at data/combined_data/AKRO_combined.csv\n",
      "Combined dataset saved for TROY at data/combined_data/TROY_combined.csv\n",
      "Combined dataset saved for PST at data/combined_data/PST_combined.csv\n",
      "Combined dataset saved for KEY at data/combined_data/KEY_combined.csv\n",
      "Combined dataset saved for RIO at data/combined_data/RIO_combined.csv\n",
      "Combined dataset saved for UMA at data/combined_data/UMA_combined.csv\n",
      "Combined dataset saved for CORN at data/combined_data/CORN_combined.csv\n",
      "Combined dataset saved for LINK at data/combined_data/LINK_combined.csv\n",
      "Combined dataset saved for BNB at data/combined_data/BNB_combined.csv\n",
      "Combined dataset saved for WTC at data/combined_data/WTC_combined.csv\n",
      "Combined dataset saved for REN at data/combined_data/REN_combined.csv\n",
      "Combined dataset saved for YAM at data/combined_data/YAM_combined.csv\n",
      "Combined dataset saved for LEND at data/combined_data/LEND_combined.csv\n",
      "Combined dataset saved for COFI at data/combined_data/COFI_combined.csv\n",
      "Combined dataset saved for LTC at data/combined_data/LTC_combined.csv\n",
      "Combined dataset saved for QLC at data/combined_data/QLC_combined.csv\n",
      "Combined dataset saved for HDAO at data/combined_data/HDAO_combined.csv\n",
      "Combined dataset saved for BAND at data/combined_data/BAND_combined.csv\n",
      "Combined dataset saved for XPR at data/combined_data/XPR_combined.csv\n",
      "Combined dataset saved for SOUL at data/combined_data/SOUL_combined.csv\n",
      "Combined dataset saved for SOL at data/combined_data/SOL_combined.csv\n",
      "Error reading data/tweets/FTI/2020-09-13.csv: No columns to parse from file\n",
      "Combined dataset saved for FTI at data/combined_data/FTI_combined.csv\n",
      "Combined dataset saved for CNTM at data/combined_data/CNTM_combined.csv\n",
      "Combined dataset saved for ZRX at data/combined_data/ZRX_combined.csv\n",
      "Combined dataset saved for OKB at data/combined_data/OKB_combined.csv\n",
      "Combined dataset saved for CTSI at data/combined_data/CTSI_combined.csv\n",
      "Combined dataset saved for DOGE at data/combined_data/DOGE_combined.csv\n",
      "Combined dataset saved for BTM at data/combined_data/BTM_combined.csv\n",
      "Combined dataset saved for BAL at data/combined_data/BAL_combined.csv\n",
      "Combined dataset saved for BCD at data/combined_data/BCD_combined.csv\n",
      "Combined dataset saved for KIN at data/combined_data/KIN_combined.csv\n",
      "Combined dataset saved for BNT at data/combined_data/BNT_combined.csv\n",
      "Combined dataset saved for MATIC at data/combined_data/MATIC_combined.csv\n"
     ]
    }
   ],
   "source": [
    "price_data_folder = \"data/price_data\"\n",
    "tweets_folder = \"data/tweets\"\n",
    "output_folder = \"data/combined_data\"\n",
    "\n",
    "create_combined_dataset(price_data_folder, tweets_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_combined_data(combined_folder):\n",
    "    combined_data = []\n",
    "    counter = 0\n",
    "    for file in os.listdir(combined_folder):\n",
    "        if counter <= 10:\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(combined_folder, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                df = df.drop(df.columns[[0, 1]], axis=1)\n",
    "                \n",
    "                if 'tweets' in df.columns:\n",
    "                    \n",
    "                    df['cryptocurrency'] = file.split(\"_\")[0]\n",
    "                    df['tweets'] = df['tweets'].apply(lambda x: eval(x) if isinstance(x, str) else [])\n",
    "\n",
    "                columns = ['cryptocurrency'] + [col for col in df.columns if col != 'cryptocurrency']\n",
    "                df = df[columns]\n",
    "                combined_data.append(df)\n",
    "            counter += 1\n",
    "    return pd.concat(combined_data, ignore_index=True)\n",
    "\n",
    "combined_folder = \"data/combined_data\"\n",
    "combined_df = load_combined_data(combined_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cryptocurrency', 'datetime', 'low_x', 'high_x', 'open_x', 'close_x',\n",
       "       'volumefrom_x', 'volumeto_x', 'label', 'tweets'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cryptocurrency</th>\n",
       "      <th>datetime</th>\n",
       "      <th>low_x</th>\n",
       "      <th>high_x</th>\n",
       "      <th>open_x</th>\n",
       "      <th>close_x</th>\n",
       "      <th>volumefrom_x</th>\n",
       "      <th>volumeto_x</th>\n",
       "      <th>label</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ESS</td>\n",
       "      <td>2018-09-05</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.009350</td>\n",
       "      <td>0.006412</td>\n",
       "      <td>248484.30</td>\n",
       "      <td>1838.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Top 5  $CDT $BTC on @hitbtc +10% $SOUL $BTC o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ESS</td>\n",
       "      <td>2018-09-06</td>\n",
       "      <td>0.005777</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.006412</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>403632.80</td>\n",
       "      <td>2509.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[How Many Essex Property Trust, Inc. $ESS's An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ESS</td>\n",
       "      <td>2018-09-07</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.006956</td>\n",
       "      <td>1187239.96</td>\n",
       "      <td>8210.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[$AIV $AVB $EQR $ESS $UDR  https://t.co/uTZePC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ESS</td>\n",
       "      <td>2018-09-08</td>\n",
       "      <td>0.006631</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.006956</td>\n",
       "      <td>0.007004</td>\n",
       "      <td>203850.09</td>\n",
       "      <td>1501.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Top 5  $UKG $ETH on @kucoincom +11% $ARN $ETH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESS</td>\n",
       "      <td>2018-09-09</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>0.007004</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>130244.30</td>\n",
       "      <td>878.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Provise Management Group Has Cut Its Lamar Ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6751</th>\n",
       "      <td>SENC</td>\n",
       "      <td>2021-04-03</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6752</th>\n",
       "      <td>SENC</td>\n",
       "      <td>2021-04-04</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[ https://t.co/BfcbXSA9tN  Use my  https://t.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6753</th>\n",
       "      <td>SENC</td>\n",
       "      <td>2021-04-05</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6754</th>\n",
       "      <td>SENC</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6755</th>\n",
       "      <td>SENC</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>38760525.36</td>\n",
       "      <td>104507.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6756 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cryptocurrency    datetime     low_x    high_x    open_x   close_x  \\\n",
       "0               ESS  2018-09-05  0.006400  0.010800  0.009350  0.006412   \n",
       "1               ESS  2018-09-06  0.005777  0.007700  0.006412  0.006100   \n",
       "2               ESS  2018-09-07  0.006100  0.008000  0.006100  0.006956   \n",
       "3               ESS  2018-09-08  0.006631  0.007990  0.006956  0.007004   \n",
       "4               ESS  2018-09-09  0.006200  0.007250  0.007004  0.006500   \n",
       "...             ...         ...       ...       ...       ...       ...   \n",
       "6751           SENC  2021-04-03  0.002304  0.002304  0.002304  0.002304   \n",
       "6752           SENC  2021-04-04  0.002304  0.002304  0.002304  0.002304   \n",
       "6753           SENC  2021-04-05  0.002304  0.002304  0.002304  0.002304   \n",
       "6754           SENC  2021-04-06  0.002304  0.002304  0.002304  0.002304   \n",
       "6755           SENC  2021-04-07  0.002201  0.003228  0.002304  0.002471   \n",
       "\n",
       "      volumefrom_x  volumeto_x  label  \\\n",
       "0        248484.30     1838.68    0.0   \n",
       "1        403632.80     2509.71    0.0   \n",
       "2       1187239.96     8210.72    0.0   \n",
       "3        203850.09     1501.02    0.0   \n",
       "4        130244.30      878.02    0.0   \n",
       "...            ...         ...    ...   \n",
       "6751          0.00        0.00    0.0   \n",
       "6752          0.00        0.00    0.0   \n",
       "6753          0.00        0.00    0.0   \n",
       "6754          0.00        0.00    0.0   \n",
       "6755   38760525.36   104507.24    0.0   \n",
       "\n",
       "                                                 tweets  \n",
       "0     [Top 5  $CDT $BTC on @hitbtc +10% $SOUL $BTC o...  \n",
       "1     [How Many Essex Property Trust, Inc. $ESS's An...  \n",
       "2     [$AIV $AVB $EQR $ESS $UDR  https://t.co/uTZePC...  \n",
       "3     [Top 5  $UKG $ETH on @kucoincom +11% $ARN $ETH...  \n",
       "4     [Provise Management Group Has Cut Its Lamar Ad...  \n",
       "...                                                 ...  \n",
       "6751                                                 []  \n",
       "6752  [ https://t.co/BfcbXSA9tN  Use my  https://t.c...  \n",
       "6753                                                 []  \n",
       "6754                                                 []  \n",
       "6755                                                 []  \n",
       "\n",
       "[6756 rows x 10 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoDataset(Dataset):\n",
    "    def __init__(self, df, features, label_column):\n",
    "\n",
    "        self.features = df[features].values\n",
    "        self.labels = df[label_column].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CryptoClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x)) \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = ['low_x', 'high_x', 'open_x', 'close_x', 'volumefrom_x', 'volumeto_x']  # Add more if needed\n",
    "label_column = 'label'\n",
    "\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = CryptoDataset(train_df, features, label_column)\n",
    "test_dataset = CryptoDataset(test_df, features, label_column)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.4395230635781924\n",
      "Epoch 2/10, Loss: 1.50074885901304\n",
      "Epoch 3/10, Loss: 1.4978626532076584\n",
      "Epoch 4/10, Loss: 1.4978166519357239\n",
      "Epoch 5/10, Loss: 1.500443506913536\n",
      "Epoch 6/10, Loss: 1.500436010610652\n",
      "Epoch 7/10, Loss: 1.4977905921156855\n",
      "Epoch 8/10, Loss: 1.497788243580378\n",
      "Epoch 9/10, Loss: 1.5004277207121863\n",
      "Epoch 10/10, Loss: 1.4977849023881031\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dim = len(features)\n",
    "hidden_dim = 128\n",
    "model = CryptoClassifier(input_dim, hidden_dim)\n",
    "criterion = nn.BCELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for features_batch, labels_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features_batch).squeeze()\n",
    "        loss = criterion(outputs, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced data exmaple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99      1335\n",
      "         1.0       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.99      1352\n",
      "   macro avg       0.49      0.50      0.50      1352\n",
      "weighted avg       0.98      0.99      0.98      1352\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1335    0]\n",
      " [  17    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilianahotsko/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/lilianahotsko/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/lilianahotsko/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features_batch, labels_batch in test_loader:\n",
    "        preds = model(features_batch).squeeze()\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "all_preds = (np.array(all_preds) > 0.5).astype(int)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(range(len(all_preds)), all_preds, label=\"Predicted\", alpha=0.7)\n",
    "plt.scatter(range(len(all_labels)), all_labels, label=\"Actual\", alpha=0.7)\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.legend()\n",
    "plt.title(\"Predicted vs. Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "axes[0].plot(range(len(all_preds)), all_preds, 'o', label=\"Predicted\", alpha=0.7)\n",
    "axes[0].set_title(\"Predicted Values\")\n",
    "axes[0].set_xlabel(\"Sample\")\n",
    "axes[0].set_ylabel(\"Value\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(range(len(all_labels)), all_labels, 'o', label=\"Actual\", alpha=0.7)\n",
    "axes[1].set_title(\"Actual Values\")\n",
    "axes[1].set_xlabel(\"Sample\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
