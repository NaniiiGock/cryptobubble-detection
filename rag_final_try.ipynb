{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilianahotsko/Desktop/tartu_4_1/transformers/CryptoBubbles-NAACL-main/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from glob import glob\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBubbleDetector:\n",
    "    def __init__(self, price_data_path: str, tweets_path: str, model_name: str = \"all-MiniLM-L6-v2\",\n",
    "                 sequence_window: int = 3):\n",
    "\n",
    "        self.price_data_path = price_data_path\n",
    "        self.tweets_path = tweets_path\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.sequence_window = sequence_window\n",
    "        self.cached_embeddings = {}\n",
    "        self.historical_data = self._load_all_historical_data()\n",
    "        self.temporal_patterns = self._create_temporal_patterns()\n",
    "        \n",
    "    def _load_all_historical_data(self) -> Dict:\n",
    "        historical_data = {}\n",
    "        \n",
    "        for file_path in glob(os.path.join(self.price_data_path, \"*.csv\")):\n",
    "            symbol = os.path.basename(file_path).replace('.csv', '')\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            historical_data[symbol] = df\n",
    "            \n",
    "        return historical_data\n",
    "    \n",
    "    def _create_temporal_patterns(self) -> Dict:\n",
    "        patterns = defaultdict(list)\n",
    "        \n",
    "        for symbol, df in self.historical_data.items():\n",
    "            df = df.sort_values('datetime')\n",
    "        \n",
    "            for i in range(len(df) - self.sequence_window + 1):\n",
    "                window = df.iloc[i:i + self.sequence_window]\n",
    "  \n",
    "                pattern_key = f\"{symbol}_{window.iloc[0]['datetime'].strftime('%Y-%m-%d')}\"\n",
    "                patterns[pattern_key] = {\n",
    "                    'dates': window['datetime'].dt.strftime('%Y-%m-%d').tolist(),\n",
    "                    'labels': window['label'].tolist(),\n",
    "                    'symbol': symbol\n",
    "                }\n",
    "                \n",
    "        return patterns\n",
    "    \n",
    "    def _process_tweets(self, tweets: List[str]) -> np.ndarray:\n",
    "        if not tweets:\n",
    "            return None\n",
    "            \n",
    "        processed_tweets = []\n",
    "        for tweet in tweets:\n",
    "            if isinstance(tweet, str):\n",
    "                tweet = ' '.join(word for word in tweet.split() if not word.startswith('http'))\n",
    "                tweet = tweet.strip()\n",
    "                if tweet:\n",
    "                    processed_tweets.append(tweet)\n",
    "                    \n",
    "        if not processed_tweets:\n",
    "            return None\n",
    "        \n",
    "        combined_text = \" \".join(processed_tweets)\n",
    "        return self.model.encode([combined_text], convert_to_tensor=True)\n",
    "    \n",
    "    def _get_historical_tweets(self, symbol: str, date: str) -> List[str]:\n",
    "        try:\n",
    "            file_path = os.path.join(self.tweets_path, symbol, f\"{date}.csv\")\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path, encoding='utf-8')\n",
    "                return df[df['tweet'].notna()]['tweet'].tolist()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading tweets for {symbol} on {date}: {str(e)}\")\n",
    "        return []\n",
    "    \n",
    "    def _calculate_sequence_similarity(self, \n",
    "                                    query_sequence: Dict[str, List[str]], \n",
    "                                    pattern: Dict) -> Tuple[float, List[float]]:\n",
    "        query_dates = sorted(query_sequence.keys())\n",
    "        pattern_dates = pattern['dates']\n",
    "        \n",
    "        if len(query_dates) != len(pattern_dates):\n",
    "            return 0.0, [0.0] * len(query_dates)\n",
    "            \n",
    "        daily_similarities = []\n",
    "        \n",
    "        for query_date, pattern_date in zip(query_dates, pattern_dates):\n",
    "            query_embedding = self._process_tweets(query_sequence[query_date])\n",
    "            \n",
    "            cache_key = f\"{pattern['symbol']}_{pattern_date}\"\n",
    "            if cache_key not in self.cached_embeddings:\n",
    "                historical_tweets = self._get_historical_tweets(pattern['symbol'], pattern_date)\n",
    "                historical_embedding = self._process_tweets(historical_tweets)\n",
    "                if historical_embedding is not None:\n",
    "                    self.cached_embeddings[cache_key] = historical_embedding\n",
    "                else:\n",
    "                    daily_similarities.append(0.0)\n",
    "                    continue\n",
    "                    \n",
    "            historical_embedding = self.cached_embeddings[cache_key]\n",
    "            \n",
    "            if query_embedding is not None and historical_embedding is not None:\n",
    "                similarity = cosine_similarity(\n",
    "                    query_embedding.cpu().numpy(),\n",
    "                    historical_embedding.cpu().numpy()\n",
    "                )[0][0]\n",
    "                daily_similarities.append(float(similarity))\n",
    "            else:\n",
    "                daily_similarities.append(0.0)\n",
    "                \n",
    "        sequence_similarity = np.mean(daily_similarities) if daily_similarities else 0.0\n",
    "        return sequence_similarity, daily_similarities\n",
    "    \n",
    "    def predict_bubble_sequence(self, \n",
    "                              tweet_sequence: Dict[str, List[str]], \n",
    "                              top_k: int = 5) -> Dict:\n",
    "\n",
    "        if len(tweet_sequence) != self.sequence_window:\n",
    "            return {\n",
    "                'error': f'Sequence must contain exactly {self.sequence_window} days of data'\n",
    "            }\n",
    "            \n",
    "        similar_patterns = []\n",
    "    \n",
    "        for pattern_key, pattern in self.temporal_patterns.items():\n",
    "            sequence_similarity, daily_similarities = self._calculate_sequence_similarity(\n",
    "                tweet_sequence, pattern\n",
    "            )\n",
    "            \n",
    "            if sequence_similarity > 0:\n",
    "                similar_patterns.append({\n",
    "                    'pattern_key': pattern_key,\n",
    "                    'symbol': pattern['symbol'],\n",
    "                    'dates': pattern['dates'],\n",
    "                    'labels': pattern['labels'],\n",
    "                    'sequence_similarity': sequence_similarity,\n",
    "                    'daily_similarities': daily_similarities\n",
    "                })\n",
    "                \n",
    "\n",
    "        similar_patterns.sort(key=lambda x: x['sequence_similarity'], reverse=True)\n",
    "        similar_patterns = similar_patterns[:top_k]\n",
    "        \n",
    "        if not similar_patterns:\n",
    "            return {\n",
    "                'error': 'No similar patterns found',\n",
    "                'predictions': []\n",
    "            }\n",
    "            \n",
    "        query_dates = sorted(tweet_sequence.keys())\n",
    "        daily_predictions = []\n",
    "        \n",
    "        for day_idx in range(len(query_dates)):\n",
    "            day_weights = []\n",
    "            day_labels = []\n",
    "            \n",
    "            for pattern in similar_patterns:\n",
    "                weight = pattern['daily_similarities'][day_idx]\n",
    "                label = pattern['labels'][day_idx]\n",
    "                day_weights.append(weight)\n",
    "                day_labels.append(label)\n",
    "                \n",
    "            if sum(day_weights) > 0:\n",
    "                bubble_probability = np.average(day_labels, weights=day_weights)\n",
    "            else:\n",
    "                bubble_probability = 0.0\n",
    "                \n",
    "            daily_predictions.append({\n",
    "                'date': query_dates[day_idx],\n",
    "                'bubble_probability': float(bubble_probability),\n",
    "                'is_bubble': bubble_probability > 0.5\n",
    "            })\n",
    "            \n",
    "        return {\n",
    "            'daily_predictions': daily_predictions,\n",
    "            'similar_patterns': [\n",
    "                {\n",
    "                    'symbol': p['symbol'],\n",
    "                    'dates': p['dates'],\n",
    "                    'labels': p['labels'],\n",
    "                    'similarity': float(p['sequence_similarity']),\n",
    "                    'daily_similarities': [float(s) for s in p['daily_similarities']]\n",
    "                }\n",
    "                for p in similar_patterns\n",
    "            ],\n",
    "            'overall_confidence': float(similar_patterns[0]['sequence_similarity']) if similar_patterns else 0.0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def example_usage():\n",
    "    detector = TemporalBubbleDetector(\n",
    "        price_data_path=\"small_data/price_data\",\n",
    "        tweets_path=\"small_data/tweets\",\n",
    "        sequence_window=3\n",
    "    )\n",
    "    \n",
    "    query_sequence = {\n",
    "        \"2024-01-01\": [\n",
    "            \"Bitcoin hitting new highs! 🚀\",\n",
    "            \"Mass adoption incoming!\"\n",
    "        ],\n",
    "        \"2024-01-02\": [\n",
    "            \"Price keeps climbing, no end in sight!\",\n",
    "            \"Everyone's talking about crypto\"\n",
    "        ],\n",
    "        \"2024-01-03\": [\n",
    "            \"Some warning signs in the market\",\n",
    "            \"Still bullish but cautious\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    result = detector.predict_bubble_sequence(query_sequence)\n",
    "    return result\n",
    "\n",
    "\n",
    "result = example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daily_predictions': [{'date': '2024-01-01',\n",
       "   'bubble_probability': 0.0,\n",
       "   'is_bubble': np.False_},\n",
       "  {'date': '2024-01-02', 'bubble_probability': 0.0, 'is_bubble': np.False_},\n",
       "  {'date': '2024-01-03', 'bubble_probability': 0.0, 'is_bubble': np.False_}],\n",
       " 'similar_patterns': [{'symbol': 'ABT',\n",
       "   'dates': ['2018-10-18', '2018-10-19', '2018-10-20'],\n",
       "   'labels': [0.0, 0.0, 0.0],\n",
       "   'similarity': 0.4368457595507304,\n",
       "   'daily_similarities': [0.3873489499092102,\n",
       "    0.4759049415588379,\n",
       "    0.44728338718414307]},\n",
       "  {'symbol': 'ABT',\n",
       "   'dates': ['2018-09-13', '2018-09-14', '2018-09-15'],\n",
       "   'labels': [0.0, 0.0, 0.0],\n",
       "   'similarity': 0.4269466797510783,\n",
       "   'daily_similarities': [0.35089048743247986,\n",
       "    0.48453769087791443,\n",
       "    0.4454118609428406]},\n",
       "  {'symbol': 'ABT',\n",
       "   'dates': ['2019-07-31', '2019-08-01', '2019-08-02'],\n",
       "   'labels': [0.0, 0.0, 0.0],\n",
       "   'similarity': 0.4240749478340149,\n",
       "   'daily_similarities': [0.39896905422210693,\n",
       "    0.41189277172088623,\n",
       "    0.4613630175590515]},\n",
       "  {'symbol': 'ABT',\n",
       "   'dates': ['2018-09-12', '2018-09-13', '2018-09-14'],\n",
       "   'labels': [0.0, 0.0, 0.0],\n",
       "   'similarity': 0.42269078890482586,\n",
       "   'daily_similarities': [0.3445132374763489,\n",
       "    0.47298663854599,\n",
       "    0.45057249069213867]},\n",
       "  {'symbol': 'ABT',\n",
       "   'dates': ['2018-07-20', '2018-07-21', '2018-07-22'],\n",
       "   'labels': [0.0, 0.0, 0.0],\n",
       "   'similarity': 0.41985994577407837,\n",
       "   'daily_similarities': [0.37195688486099243,\n",
       "    0.5017317533493042,\n",
       "    0.3858911991119385]}],\n",
       " 'overall_confidence': 0.4368457595507304}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of input to llm with small data from matched embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a financial market expert. Below are the results of a cryptocurrency bubble prediction model:\n",
      "\n",
      "Predictions for the next 3 days:\n",
      "Date: 2024-01-01, Bubble Probability: 0.0, Is Bubble: False\n",
      "Date: 2024-01-02, Bubble Probability: 0.0, Is Bubble: False\n",
      "Date: 2024-01-03, Bubble Probability: 0.0, Is Bubble: False\n",
      "\n",
      "Similar patterns found in the historical data:\n",
      "Symbol: ABT, Dates: 2018-10-18, 2018-10-19, 2018-10-20, Labels: 0.0, 0.0, 0.0, Similarity: 0.44, Daily Similarities: 0.39, 0.48, 0.45\n",
      "\n",
      "Overall Confidence: 0.44\n",
      "\n",
      "Please analyze the results, identify any patterns or insights, and provide an interpretation of the predictions.\n"
     ]
    }
   ],
   "source": [
    "def create_llm_input(result):\n",
    "    daily_predictions = result['daily_predictions']\n",
    "    similar_patterns = result['similar_patterns']\n",
    "    overall_confidence = result['overall_confidence']\n",
    "    \n",
    "    prompt = (\n",
    "        \"You are a financial market expert. Below are the results of a cryptocurrency bubble prediction model:\\n\\n\"\n",
    "        \"Predictions for the next 3 days:\\n\"\n",
    "    )\n",
    "    \n",
    "    for prediction in daily_predictions:\n",
    "        date = prediction['date']\n",
    "        bubble_probability = prediction['bubble_probability']\n",
    "        is_bubble = prediction['is_bubble']\n",
    "        prompt += f\"Date: {date}, Bubble Probability: {bubble_probability}, Is Bubble: {is_bubble}\\n\"\n",
    "    \n",
    "    prompt += \"\\nSimilar patterns found in the historical data:\\n\"\n",
    "    \n",
    "    for pattern in similar_patterns:\n",
    "        symbol = pattern['symbol']\n",
    "        dates = \", \".join(pattern['dates'])\n",
    "        labels = \", \".join(str(label) for label in pattern['labels'])\n",
    "        similarity = pattern['similarity']\n",
    "        daily_similarities = \", \".join(f\"{sim:.2f}\" for sim in pattern['daily_similarities'])\n",
    "        prompt += (f\"Symbol: {symbol}, Dates: {dates}, Labels: {labels}, \"\n",
    "                   f\"Similarity: {similarity:.2f}, Daily Similarities: {daily_similarities}\\n\")\n",
    "    \n",
    "    prompt += f\"\\nOverall Confidence: {overall_confidence:.2f}\\n\\n\"\n",
    "    prompt += \"Please analyze the results, identify any patterns or insights, and provide an interpretation of the predictions.\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "result = {\n",
    "    'daily_predictions': [{'date': '2024-01-01', 'bubble_probability': 0.0, 'is_bubble': np.False_},\n",
    "                          {'date': '2024-01-02', 'bubble_probability': 0.0, 'is_bubble': np.False_},\n",
    "                          {'date': '2024-01-03', 'bubble_probability': 0.0, 'is_bubble': np.False_}],\n",
    "    'similar_patterns': [{'symbol': 'ABT', 'dates': ['2018-10-18', '2018-10-19', '2018-10-20'],\n",
    "                          'labels': [0.0, 0.0, 0.0], 'similarity': 0.4368457595507304,\n",
    "                          'daily_similarities': [0.3873489499092102, 0.4759049415588379, 0.44728338718414307]}],\n",
    "    'overall_confidence': 0.4368457595507304\n",
    "}\n",
    "\n",
    "llm_input = create_llm_input(result)\n",
    "\n",
    "print(llm_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
